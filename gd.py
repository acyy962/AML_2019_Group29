{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9pwShhHJ9Zhf","colab_type":"text"},"source":["##Algorithms for gradient descent in two dimensions"]},{"cell_type":"markdown","metadata":{"id":"q9eNiHZQ9fZc","colab_type":"text"},"source":["## Import libraries"]},{"cell_type":"code","metadata":{"id":"5bNm6ONw9T4l","colab_type":"code","colab":{}},"source":["import numpy as np\n","import math"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NM8ZtxBf9lgO","colab_type":"text"},"source":["## Gradient descent algorithms\n","\n","First:\n","* Choose starting points for X1 and X2\n","* Find the gradients at the starting point\n","* If the Euclidean norm of the two-dimentional derivatives of loss function is close to zero then stop\n","* Otherwise iterate\n","\n","Iterate as follows:\n","* Take small steps in the downhill direction (opposite to the gradient)\n","* Find the gradients\n","* If the Euclidean norm of the two-dimentional derivatives of loss function is close to zero then stop"]},{"cell_type":"code","metadata":{"id":"7rDRYBMk9RgV","colab_type":"code","colab":{}},"source":["class gd_2d:                                          #define a class gd_2d\n","    \n","    def __init__(self, fn_loss, fn_grad1, fn_grad2):  #using the __init__ to initialize the coding class\n","        self.fn_loss = fn_loss                        #involve 'self' to pass the method a reference to the parent class\n","        self.fn_grad1 = fn_grad1\n","        self.fn_grad2 = fn_grad2\n","        \n","    def pv(self, x1_init, x2_init, n_iter, eta, tol, tol_upper):  #define plain vanilla gradient descent method\n","        \n","        x1 = x1_init                                  #define variable x1\n","        x2 = x2_init                                  #define variable x2\n","        \n","        loss_path = []                                #initialise lists to score the path of x1, x2, and the path of the loss function\n","        x1_path = []\n","        x2_path = []\n","        \n","        x1_path.append(x1)\n","        x2_path.append(x2)\n","        \n","        loss_this = self.fn_loss(x1, x2)\n","        loss_path.append(loss_this)\n","        g1 = self.fn_grad1(x1, x2)\n","        g2 = self.fn_grad2(x1, x2)\n","        \n","        for i in range(n_iter):\n","            if  math.sqrt(g1**2 + g2**2) < tol or loss_this > tol_upper:\n","                break\n","            g1 = self.fn_grad1(x1, x2)\n","            g2 = self.fn_grad2(x1, x2)\n","            x1 += -eta * g1\n","            x1_path.append(x1)\n","            x2 += -eta * g2\n","            x2_path.append(x2)\n","            loss_this = self.fn_loss(x1, x2)\n","            loss_path.append(loss_this)\n","            \n","        if loss_this > tol_upper:\n","            print('Exploded')\n","        elif math.sqrt(g1**2 + g2**2) > tol:\n","            print('Did not converge')\n","        else:\n","            print('Converged in {} steps.  Loss fn {} achieved by x1 = {} x2 = {}'.format(i, loss_this, x1, x2))\n","        self.loss_path = np.array(loss_path)\n","        self.x1_path = np.array(x1_path)\n","        self.x2_path = np.array(x2_path)\n","        \n","    def momentum(self, x1_init, x2_init, n_iter, eta, tol, tol_upper, alpha):\n","   \n","        x1 = x1_init\n","        x2 = x2_init\n","        \n","        loss_path = []\n","        x1_path = []\n","        x2_path = []\n","        nu1_path = []\n","        \n","        x1_path.append(x1)\n","        x2_path.append(x2)\n","        loss_this = self.fn_loss(x1, x2)\n","        loss_path.append(loss_this)\n","        g1 = self.fn_grad1(x1, x2)\n","        g2 = self.fn_grad2(x1, x2)\n","        nu1 = 0\n","        nu1_path.append(nu1)\n","        nu2 = 0\n","        \n","        for i in range(n_iter):\n","            g1 = self.fn_grad1(x1, x2)\n","            g2 = self.fn_grad2(x1, x2)\n","            \n","            if math.sqrt(g1**2 + g2**2) < tol or loss_this > tol_upper:\n","                break\n","\n","            nu1 = alpha * nu1 + eta * g1\n","            nu1_path.append(nu1)\n","            nu2 = alpha * nu2 + eta * g2\n","            x1 += -nu1\n","            x1_path.append(x1)\n","            x2 += -nu2\n","            x2_path.append(x2)\n","            loss_this = self.fn_loss(x1, x2)\n","            loss_path.append(loss_this)\n","\n","        if loss_this > tol_upper:\n","            print('Exploded')\n","        elif math.sqrt(g1**2 + g2**2) > tol:\n","            print('Did not converge')\n","        else:\n","            print('Converged in {} steps.  Loss fn {} achieved by x1 = {} x2 = {}'.format(i, loss_this, x1, x2))\n","        self.loss_path = np.array(loss_path)\n","        self.x1_path = np.array(x1_path)\n","        self.x2_path = np.array(x2_path)\n","    \n","    def nag(self, x1_init, x2_init, n_iter, eta, tol, tol_upper, alpha):\n","        x1 = x1_init\n","        x2 = x2_init\n","        \n","        loss_path = []\n","        x1_path = []\n","        x2_path = []\n","        \n","        x1_path.append(x1)\n","        x2_path.append(x2)\n","        loss_this = self.fn_loss(x1, x2)\n","        loss_path.append(loss_this)\n","        g1 = self.fn_grad1(x1, x2)\n","        g2 = self.fn_grad2(x1, x2)\n","        nu1 = 0\n","        nu2 = 0\n","\n","        for i in range(n_iter):\n","            # i starts from 0 so add 1\n","            # The formula for mu was mentioned by David Barber UCL as being Nesterovs suggestion\n","            mu = 1 - 3 / (i + 1 + 5) \n","            g1 = self.fn_grad1(x1 - mu*nu1, x2 - mu*nu2)\n","            g2 = self.fn_grad2(x1 - mu*nu1, x2 - mu*nu2)\n","            \n","            if math.sqrt(g1**2 + g2**2) < tol or loss_this > tol_upper:\n","                break\n","\n","            nu1 = alpha * nu1 + eta * g1\n","            nu2 = alpha * nu2 + eta * g2\n","            x1 += -nu1\n","            x1_path.append(x1)\n","            x2 += -nu2\n","            x2_path.append(x2)\n","            loss_this = self.fn_loss(x1, x2)\n","            loss_path.append(loss_this)\n","            \n","        if loss_this > tol_upper:\n","            print('Exploded')\n","        elif math.sqrt(g1**2 + g2**2) > tol:\n","            print('Did not converge')\n","        else:\n","            print('Converged in {} steps.  Loss fn {} achieved by x1 = {} x2 = {}'.format(i, loss_this, x1, x2))\n","        self.loss_path = np.array(loss_path)\n","        self.x1_path = np.array(x1_path)\n","        self.x2_path = np.array(x2_path)"],"execution_count":0,"outputs":[]}]}